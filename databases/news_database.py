import sqlite3
import aiohttp
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import logging

logger = logging.getLogger(__name__)

class NewsDatabase:
    def __init__(self, db_path="news.db"):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                url TEXT UNIQUE NOT NULL,
                date TEXT,
                category TEXT,
                content_text TEXT,
                full_content TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_search_index (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                article_id INTEGER,
                keywords TEXT,
                FOREIGN KEY (article_id) REFERENCES news_articles (id)
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS update_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                articles_processed INTEGER,
                new_articles INTEGER
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def needs_update(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT last_update FROM update_log ORDER BY id DESC LIMIT 1')
        result = cursor.fetchone()
        conn.close()
        
        if not result:
            return True
        
        last_update = datetime.fromisoformat(result[0])
        return datetime.now() - last_update > timedelta(hours=1)
    
    async def parse_article_content(self, url):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        content_div = soup.find('div', class_='article-content')
                        if content_div:
                            full_content = content_div.get_text(separator='\n', strip=True)
                            
                            cleaned_content = re.sub(r'\s+', ' ', full_content).strip()
             
                            preview_content = cleaned_content[:300] + "..." if len(cleaned_content) > 300 else cleaned_content
                            
                            return {
                                'full_content': cleaned_content,
                                'preview_content': preview_content
                            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç–∞—Ç—å–∏ {url}: {str(e)}")
        
        return None
    
    def save_article(self, article_data):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('SELECT id FROM news_articles WHERE url = ?', (article_data['url'],))
            existing = cursor.fetchone()
            
            if existing:
                cursor.execute('''
                    UPDATE news_articles 
                    SET title = ?, date = ?, category = ?, content_text = ?, full_content = ?, updated_at = CURRENT_TIMESTAMP
                    WHERE url = ?
                ''', (
                    article_data['title'],
                    article_data.get('date'),
                    article_data.get('category'),
                    article_data.get('preview_content'),
                    article_data.get('full_content'),
                    article_data['url']
                ))
                article_id = existing[0]
            else:
                cursor.execute('''
                    INSERT INTO news_articles (title, url, date, category, content_text, full_content)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    article_data['title'],
                    article_data['url'],
                    article_data.get('date'),
                    article_data.get('category'),
                    article_data.get('preview_content'),
                    article_data.get('full_content')
                ))
                article_id = cursor.lastrowid
            
            cursor.execute('DELETE FROM news_search_index WHERE article_id = ?', (article_id,))

            search_text = f"{article_data['title']} {article_data.get('category', '')} {article_data.get('preview_content', '')}"
            keywords = ' '.join(re.findall(r'\b\w{3,}\b', search_text.lower()))
            
            cursor.execute('''
                INSERT INTO news_search_index (article_id, keywords)
                VALUES (?, ?)
            ''', (article_id, keywords))
            
            conn.commit()
            return article_id
            
        except Exception as e:
            conn.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏: {str(e)}")
            return None
        finally:
            conn.close()
    
    def log_update(self, processed_count, new_articles_count):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO update_log (articles_processed, new_articles)
            VALUES (?, ?)
        ''', (processed_count, new_articles_count))
        
        conn.commit()
        conn.close()
    
    def search_articles(self, keyword, limit=1100):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                na.title,
                na.url,
                na.date,
                na.category,
                na.content_text,
                na.full_content
            FROM news_articles na
            JOIN news_search_index nsi ON na.id = nsi.article_id
            WHERE na.title LIKE ? OR nsi.keywords LIKE ? OR na.category LIKE ?
            ORDER BY na.created_at DESC
            LIMIT ?
        ''', (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%', limit))
        
        results = cursor.fetchall()
        conn.close()
        
        articles = []
        for title, url, date, category, content_text, full_content in results:
            articles.append({
                'title': title,
                'url': url,
                'date': date,
                'category': category,
                'preview_content': content_text,
                'full_content': full_content
            })
        
        return articles
    
    def get_stats(self):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM news_articles')
        total_articles = cursor.fetchone()[0]
        
        cursor.execute('SELECT last_update FROM update_log ORDER BY id DESC LIMIT 1')
        last_update_result = cursor.fetchone()
        last_update = last_update_result[0] if last_update_result else None
        
        conn.close()
        
        return {
            'total_articles': total_articles,
            'last_update': last_update
        }
    def article_exists(self, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º URL –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT id FROM news_articles WHERE url = ?', (url,))
        exists = cursor.fetchone() is not None
        conn.close()
        return exists

async def fetch_news_from_source(max_pages=50):
    from articles_search import parse_news_card
    import requests
    
    found_articles = []
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞...")
    
    for page in range(1, max_pages + 1):
        try:
            url = f"https://mir.pravo.by/news/?PAGEN_1={page}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_container = soup.find('div', id='news-container')
            
            if not news_container:
                if page >= 3:
                    break
                continue

            news_cards = news_container.find_all('div', class_='news-single-item')

            if not news_cards:
                break

            for card in news_cards:
                article_data = parse_news_card(card)
                if article_data:
                    if news_db.article_exists(article_data['url']):
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è —Å—Ç–∞—Ç—å—è, –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(found_articles)} –Ω–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π")
                        return found_articles
                    
                    found_articles.append(article_data)

            if len(found_articles) >= 1100:
                break
                
        except Exception as e:
            continue
    
    logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(found_articles)} —Å—Ç–∞—Ç–µ–π")
    return found_articles

async def update_news_database():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    if not news_db.needs_update():
        logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∞–∫—Ç—É–∞–ª—å–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ")
        return {
            'processed': 0,
            'new_articles': 0,
            'status': 'already_updated'
        }
    
    logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")

    articles = await fetch_news_from_source(max_pages=50)
    
    total_processed = 0
    total_new = 0
    
    for article in articles:
        try:
            content_data = await news_db.parse_article_content(article['url'])
            if content_data:
                article.update(content_data)
            
            article_id = news_db.save_article(article)
            if article_id:
                total_new += 1
            
            total_processed += 1
            
        except Exception as e:
            continue
    
    news_db.log_update(total_processed, total_new)
    logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed}, –ù–æ–≤—ã—Ö: {total_new}")
    
    return {
        'processed': total_processed,
        'new_articles': total_new,
        'status': 'updated'
    }

def search_news_in_database(keyword, limit=1100):
    return news_db.search_articles(keyword, limit)

news_db = NewsDatabase()