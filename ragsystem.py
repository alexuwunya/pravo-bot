from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance, PointStruct
import requests
from qdrant_manager import qdrant_manager
import re
import logging

logger = logging.getLogger(__name__)

class ModelManager:
    _instance = None
    _model = None

    @classmethod
    def get_model(cls):
        if cls._model is None:
            logger.info("üîß –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å SentenceTransformer...")
            try:
                cls._model = SentenceTransformer("intfloat/multilingual-e5-large")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                raise
        return cls._model

class RAGSystem:
    def __init__(self, paragraph, collection_name="constitution_articles", document_name='–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è'):
        try:
            self.model = ModelManager.get_model()
            self.client = qdrant_manager.get_client()
            self.api_key = "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"
            self.api_url = "https://openrouter.ai/api/v1/chat/completions"
            self.llm_model = "google/gemma-3-27b-it:free"
            self.codex = paragraph
            self.document_name = document_name
            self.collection_name = collection_name

            logger.info(f"üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã –¥–ª—è: {self.document_name}")
            logger.info(f"üìù –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(self.codex)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üìö –ö–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")

            # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            self._validate_input_text()
            self.create_embeddings_if_not_exists()

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ RAGSystem: {e}")
            raise

    def _validate_input_text(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if not self.codex or len(self.codex.strip()) < 100:
            raise ValueError("–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–µ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ
        if "–û –ø—Ä–∞–≤–∞—Ö —Ä–µ–±–µ–Ω–∫–∞" in self.codex and "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è" in self.document_name:
            logger.error("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –í —Ç–µ–∫—Å—Ç –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏ –ø–æ–ø–∞–ª –∑–∞–∫–æ–Ω –æ –ø—Ä–∞–≤–∞—Ö —Ä–µ–±–µ–Ω–∫–∞!")
            raise ValueError("–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞")

    def get_articles_chunks(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        logger.info(f"üîß –†–∞–∑–±–∏–≤–∞—é —Ç–µ–∫—Å—Ç {self.document_name} –Ω–∞ —á–∞–Ω–∫–∏...")

        if not self.codex:
            logger.error("–¢–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return []

        # –î–ª—è –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏
        if self.document_name == '–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –†–µ—Å–ø—É–±–ª–∏–∫–∏ –ë–µ–ª–∞—Ä—É—Å—å':
            return self._get_constitution_chunks()
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –º–µ—Ç–æ–¥
            return self._get_general_chunks()

    def _get_constitution_chunks(self):
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏"""
        try:
            chunks = []
            current_title = "–ü—Ä–µ–∞–º–±—É–ª–∞"

            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
            sections = re.split(r'(–†–ê–ó–î–ï–õ [IVXLCDM]+|–ì–õ–ê–í–ê \d+)', self.codex)

            for i, section in enumerate(sections):
                if i == 0 and section.strip():
                    # –ü—Ä–µ–∞–º–±—É–ª–∞
                    chunks.append(self._create_chunk(section.strip(), "–ü—Ä–µ–∞–º–±—É–ª–∞", 0))
                elif i % 2 == 1:
                    # –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞/–≥–ª–∞–≤—ã
                    current_title = section.strip()
                elif section.strip():
                    # –¢–µ–∫—Å—Ç —Ä–∞–∑–¥–µ–ª–∞
                    self._process_section_text(section.strip(), current_title, chunks)

            # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
            if len(chunks) <= 1:
                logger.warning("üîß –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É—é —Ä–µ–∑–µ—Ä–≤–Ω—ã–π...")
                chunks = self._get_constitution_chunks_fallback()

            logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏")
            return chunks

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–≤–∫–µ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏: {e}")
            return self._get_fallback_chunk()

    def _process_section_text(self, section_text, current_title, chunks):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç —Ä–∞–∑–¥–µ–ª–∞ –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —Å—Ç–∞—Ç—å–∏"""
        articles = re.split(r'–°—Ç–∞—Ç—å—è \d+', section_text)
        for j, article in enumerate(articles):
            if article.strip():
                if j == 0 and "–°—Ç–∞—Ç—å—è" not in section_text[:100]:
                    article_title = current_title
                else:
                    article_title = f"{current_title}, –°—Ç–∞—Ç—å—è {j}"

                chunks.append(self._create_chunk(article.strip(), article_title, j))

    def _get_constitution_chunks_fallback(self):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏ –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏"""
        try:
            chunks = []
            parts = re.split(r'(–°—Ç–∞—Ç—å—è \d+)', self.codex)

            for i in range(0, len(parts)):
                if i == 0 and parts[i].strip():
                    chunks.append(self._create_chunk(parts[i].strip(), "–ü—Ä–µ–∞–º–±—É–ª–∞", 0))
                elif i % 2 == 1 and i + 1 < len(parts):
                    article_title = parts[i].strip()
                    article_text = parts[i + 1].strip()
                    if article_text:
                        chunks.append(self._create_chunk(article_text, article_title, len(chunks)))
            return chunks
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ä–µ–∑–µ—Ä–≤–Ω–æ–º –º–µ—Ç–æ–¥–µ —Ä–∞–∑–±–∏–≤–∫–∏: {e}")
            return self._get_fallback_chunk()

    def _get_general_chunks(self):
        """–û–±—â–∏–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ª—é–±—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            chunks = []
            lines = self.codex.split('\n')
            current_title = f"–†–∞–∑–¥–µ–ª {self.document_name}"
            current_text = ""

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # –ï—Å–ª–∏ –Ω–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ —Ä–∞–∑–¥–µ–ª–∞
                if any(line.startswith(keyword) for keyword in ['–°—Ç–∞—Ç—å—è', '–ì–ª–∞–≤–∞', '–†–ê–ó–î–ï–õ']):
                    if current_text:
                        chunks.append(self._create_chunk(current_text.strip(), current_title, 0))
                    current_title = line
                    current_text = ""
                else:
                    current_text += line + " "

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
            if current_text:
                chunks.append(self._create_chunk(current_text.strip(), current_title, 0))

            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π —á–∞–Ω–∫
            if not chunks:
                chunks = [self._get_fallback_chunk()]

            logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è {self.document_name}")
            return chunks

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—â–µ–π —Ä–∞–∑–±–∏–≤–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
            return [self._get_fallback_chunk()]

    def _create_chunk(self, text, title, index):
        """–°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∞–Ω–∫"""
        return {
            'text': text,
            'article_source': self.document_name,
            'article_title': title,
            'paragraph_index': index
        }

    def _get_fallback_chunk(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—ã–π —á–∞–Ω–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        return self._create_chunk(
            self.codex[:2000] if self.codex else "–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç",
            self.document_name,
            0
        )

    def create_embeddings_if_not_exists(self):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
        try:
            logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä—è—é –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name}...")
            if not self.client.collection_exists(self.collection_name):
                logger.info(f"üîß –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name}...")
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
                )
                self.create_embeddings()
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —Å–æ–∑–¥–∞–Ω–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            else:
                logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}: {e}")
            raise

    def create_embeddings(self):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤"""
        try:
            paragraphs = self.get_articles_chunks()
            if not paragraphs:
                logger.error("‚ùå –ù–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                return

            # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–µ—Ä–≤–æ–º —á–∞–Ω–∫–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if paragraphs:
                first_chunk_preview = paragraphs[0]['text'][:100] + "..." if len(paragraphs[0]['text']) > 100 else paragraphs[0]['text']
                logger.info(f"üîç –ü–µ—Ä–≤—ã–π —á–∞–Ω–∫: {first_chunk_preview}")

            articles = [f"passage: {paragraph['text']}" for paragraph in paragraphs]

            logger.info(f"üîß –ö–æ–¥–∏—Ä—É—é {len(articles)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –¥–ª—è {self.document_name}...")
            embeddings = self.model.encode(articles, normalize_embeddings=True)
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")

            points = []
            for i, (paragraph, vector) in enumerate(zip(paragraphs, embeddings)):
                points.append(PointStruct(
                    id=i,
                    vector=vector.tolist(),
                    payload={
                        "text": paragraph['text'],
                        "article_title": paragraph['article_title'],
                        "article_source": paragraph['article_source'],
                        "paragraph_index": paragraph['paragraph_index']
                    }
                ))

            logger.info(f"üîß –°–æ—Ö—Ä–∞–Ω—è—é {len(points)} —Ç–æ—á–µ–∫ –≤ Qdrant...")
            self.client.upsert(collection_name=self.collection_name, points=points)
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(points)} —Ç–æ—á–µ–∫ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é {self.collection_name}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {self.document_name}: {e}")
            raise

    def search_relevant_chunks(self, question, limit=5):
        """–ò—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞"""
        try:
            logger.info(f"üîç –ò—â—É –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}: {question}")
            query_text = f"query: {question}"
            query_vector = self.model.encode(query_text, normalize_embeddings=True)

            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=limit
            )
            hits = search_result.points

            context_parts = []
            for i, hit in enumerate(hits, 1):
                payload = hit.payload
                context_parts.append(f"[–î–æ–∫—É–º–µ–Ω—Ç {i}] {payload['article_title']}: {payload['text']}")

            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(context_parts)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            return "\n\n".join(context_parts)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ {self.collection_name}: {e}")
            return ""

    def ask_llm(self, question, context):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ LLM"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://t.me/detpravo_bot",
            "X-Title": "RAG Assistant"
        }

        system_prompt = f"""–¢—ã ‚Äî —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ {self.document_name}. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞–≤–∞—Ç—å —Ç–æ—á–Ω—ã–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ {self.document_name}.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
        1. **–ò—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:** –î–ª—è –æ—Ç–≤–µ—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç {self.document_name}, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∑–∞–ø—Ä–æ—Å–∞.
        2. **–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç:** –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ {self.document_name} –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–∂–µ –±–ª–∏–∑–∫–æ–π –∫ —Ç–µ–º–µ –≤–æ–ø—Ä–æ—Å–∞, —Å—Ç—Ä–æ–≥–æ —Å–∫–∞–∂–∏: ¬´–í {self.document_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å¬ª.
        3. **–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏:** –ó–∞–ø—Ä–µ—â–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ ¬´—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã¬ª, ¬´–∫–æ–Ω—Ç–µ–∫—Å—Ç¬ª, ¬´–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ¬ª –∏–ª–∏ –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞, —Ä–∞—Å–∫—Ä—ã–≤–∞—é—â–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º —Ç–≤–æ–µ–π —Ä–∞–±–æ—Ç—ã.
        4. **–ß–∏—Å—Ç–æ—Ç–∞ –æ—Ç–≤–µ—Ç–∞:** –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Markdown-—Ä–∞–∑–º–µ—Ç–∫—É.

        –ü–†–ê–í–ò–õ–ê –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–Ø –û–¢–í–ï–¢–ê:
        - –û—Ç–≤–µ—á–∞–π —á–µ—Ç–∫–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        - –í —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—Ä–∞—Ç–∫–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å—É—Ç—å –æ—Ç–≤–µ—Ç–∞.
        - –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∑—ã–≤–∞–π –Ω–æ–º–µ—Ä–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è.
        - –ò–∑–ª–∞–≥–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–µ–∑–∏—Å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∞–±–∑–∞—Ü—ã –∏ —Å–ø–∏—Å–∫–∏.
        """

        user_content = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ {self.document_name}:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∞–π —á–µ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç:"""

        data = {
            "model": self.llm_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "max_tokens": 1024
        }

        try:
            logger.info(f"üîç –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ LLM —Å –≤–æ–ø—Ä–æ—Å–æ–º: {question}")
            response = requests.post(self.api_url, headers=headers, json=data, timeout=30)
            response.raise_for_status()

            result = response.json()
            answer = result['choices'][0]['message']['content']
            logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç LLM, –¥–ª–∏–Ω–∞: {len(answer)} —Å–∏–º–≤–æ–ª–æ–≤")
            return answer

        except requests.exceptions.Timeout:
            logger.error("‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM")
            return "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–µ—Ä–≤–∏—Å–∞."
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM: {e}")
            return f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ —Å–µ—Ä–≤–∏—Å—É: {str(e)}"
        except Exception as e:
            logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM: {e}")
            return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ —Å–µ—Ä–≤–∏—Å—É: {str(e)}"

    def answer_question(self, question):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        try:
            logger.info(f"üîç –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {question}")
            context = self.search_relevant_chunks(question)
            if not context:
                logger.warning("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç")
                return f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ {self.document_name}."

            logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω, –¥–ª–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
            answer = self.ask_llm(question, context)
            return answer

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {e}")
            return f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

class LegalRAGSystem(RAGSystem):
    def __init__(self, paragraph, document_name):
        collection_name = f"legal_document_{document_name.lower().replace(' ', '_')}"
        super().__init__(paragraph, collection_name, document_name)