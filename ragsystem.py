import os
import re
import logging
import asyncio
from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance, PointStruct
from qdrant_manager import qdrant_manager
from groq import AsyncGroq

logger = logging.getLogger(__name__)

class ModelManager:
    _model = None

    @classmethod
    def get_model(cls):
        if cls._model is None:
            logger.info("üîß –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å SentenceTransformer (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            try:
                cls._model = SentenceTransformer("intfloat/multilingual-e5-large")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å SentenceTransformer –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                raise
        return cls._model


class RAGSystem:
    def __init__(self, paragraph: str, collection_name: str, document_name: str):
        self.api_key = os.getenv("GROQ_API_KEY")
        if not self.api_key:
            logger.warning("‚ö†Ô∏è GROQ_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω!")

        self.client = qdrant_manager.get_client()
        self.groq_client = AsyncGroq(api_key=self.api_key)
        self.llm_model = "llama-3.3-70b-versatile"
        self.codex = paragraph
        self.document_name = document_name

        safe_suffix = re.sub(r'[^a-zA-Z0-9_]', '', collection_name.lower().replace(' ', '_'))
        self.collection_name = f"legal_docs_{safe_suffix}" if not collection_name.startswith(
            "legal_") else collection_name

        self.model = None

    async def initialize(self):
        logger.info(f"üîç Init RAG –¥–ª—è: {self.document_name}, –ö–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")
        loop = asyncio.get_running_loop()
        self.model = await loop.run_in_executor(None, ModelManager.get_model)

        if not self.codex or len(self.codex.strip()) < 100:
            logger.error(f"‚ùå –¢–µ–∫—Å—Ç –¥–ª—è {self.document_name} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø—É—Å—Ç.")
            return False

        await self.create_embeddings_if_not_exists()
        return True

    def get_articles_chunks(self):
        if not self.codex: return []

        # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï REGEX:
        # 1. (?i) - –∏–≥–Ω–æ—Ä —Ä–µ–≥–∏—Å—Ç—Ä–∞
        # 2. \s+ - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞ –°—Ç–∞—Ç—å—è
        # 3. \d+ - –Ω–æ–º–µ—Ä
        # 4. \s* - (–í–ê–ñ–ù–û) –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ —Ç–æ—á–∫–æ–π
        # 5. \.? - –Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —Ç–æ—á–∫–∞
        split_pattern = r'(?i)((?:–°—Ç–∞—Ç—å—è|–ì–õ–ê–í–ê)\s+\d+\s*\.?|–†–ê–ó–î–ï–õ\s+[IVX]+)'

        raw_chunks = re.split(split_pattern, self.codex)

        chunks = []
        current_header = self.document_name

        # –ï—Å–ª–∏ —Å–ø–ª–∏—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª (–º–∞–ª–æ —á–∞—Å—Ç–µ–π), –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        if len(raw_chunks) < 2: raw_chunks = [self.codex]

        for i in range(len(raw_chunks)):
            segment = raw_chunks[i].strip()
            if not segment: continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–µ–≥–º–µ–Ω—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–º (–ø–æ —Ç–æ–º—É –∂–µ –ø–∞—Ç—Ç–µ—Ä–Ω—É)
            if re.fullmatch(split_pattern, segment):
                current_header = segment
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä "–°—Ç–∞—Ç—å—è 4\n." -> "–°—Ç–∞—Ç—å—è 4.")
                current_header = re.sub(r'\s+', ' ', current_header).strip()
            else:
                full_text = f"{current_header}\n{segment}"
                if len(full_text) > 3000: full_text = full_text[:3000] + "..."

                chunks.append({
                    'text': full_text,
                    'article_source': self.document_name,
                    'article_title': current_header,
                    'paragraph_index': len(chunks)
                })

        logger.info(f"üìÑ –†–∞–∑–±–∏–≤ —Ç–µ–∫—Å—Ç {self.document_name}: –ø–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        return chunks

    async def create_embeddings_if_not_exists(self):
        try:
            loop = asyncio.get_running_loop()
            exists = await loop.run_in_executor(None, lambda: self.client.collection_exists(self.collection_name))

            if not exists:
                logger.info(f"üîß –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant: {self.collection_name}")
                await loop.run_in_executor(
                    None,
                    lambda: self.client.create_collection(
                        collection_name=self.collection_name,
                        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
                    )
                )
                await self.create_embeddings()
            else:
                count_result = await loop.run_in_executor(None, lambda: self.client.count(self.collection_name))
                if count_result.count == 0:
                    await self.create_embeddings()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant Init: {e}")

    async def create_embeddings(self):
        try:
            chunks = self.get_articles_chunks()
            if not chunks:
                return

            texts = [f"passage: {c['text']}" for c in chunks]
            logger.info(f"‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è {self.document_name} ({len(texts)} —à—Ç)...")

            loop = asyncio.get_running_loop()
            embeddings = await loop.run_in_executor(
                None,
                lambda: self.model.encode(texts, normalize_embeddings=True)
            )

            points = [
                PointStruct(id=i, vector=vector.tolist(), payload=chunk)
                for i, (chunk, vector) in enumerate(zip(chunks, embeddings))
            ]

            await loop.run_in_executor(
                None,
                lambda: self.client.upsert(collection_name=self.collection_name, points=points)
            )
            logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ {self.collection_name}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")

    async def search_relevant_chunks_async(self, question, limit=4):
        loop = asyncio.get_running_loop()

        def _encode_and_search():
            query_vector = self.model.encode(f"query: {question}", normalize_embeddings=True)
            return self.client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=limit
            )

        try:
            result = await loop.run_in_executor(None, _encode_and_search)
            context_parts = [f"--- {hit.payload.get('article_title', '–û—Ç—Ä—ã–≤–æ–∫')} ---\n{hit.payload.get('text', '')}" for
                             hit in result.points]
            return "\n\n".join(context_parts)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""

    async def answer_question(self, question):
        try:
            if not self.model:
                await self.initialize()

            context = await self.search_relevant_chunks_async(question)
            if not context:
                return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ."

            system_prompt = (
                f"–¢—ã —é—Ä–∏—Å—Ç-–ø–µ–¥–∞–≥–æ–≥ –¥–ª—è –¥–µ—Ç–µ–π –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É: {self.document_name}. "
                "–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ, –ø–æ–Ω—è—Ç–Ω–æ –¥–ª—è –¥–µ—Ç–µ–π, —É–ø—Ä–æ—â–∞—è –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏. –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ 150-300 —Å–∏–º–≤–æ–ª–æ–≤"
                "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –ª–∞—Ç–∏–Ω–∏—Ü—É"
                "–ú–æ–¥–µ—à—å —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞—Ç–µ–π –∏–ª–∏ –≥–ª–∞–≤, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. "
                "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∂–∏—Ä–Ω—ã–π, –∫—É—Ä—Å–∏–≤), –ø–∏—à–∏ –æ–±—ã—á–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ —á–∞—Ç–µ Telegram"
            )
            user_content = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}"

            chat_completion = await self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                model=self.llm_model,
                temperature=0.3,
            )
            return chat_completion.choices[0].message.content

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ LLM (Groq): {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."