import os
import re
import logging
import json
import aiohttp
import asyncio
from sentence_transformers import SentenceTransformer
from qdrant_client.models import VectorParams, Distance, PointStruct
from qdrant_manager import qdrant_manager

logger = logging.getLogger(__name__)


class ModelManager:
    _model = None

    @classmethod
    def get_model(cls):
        if cls._model is None:
            logger.info("üîß –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å SentenceTransformer (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
            try:
                cls._model = SentenceTransformer("intfloat/multilingual-e5-large")
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å SentenceTransformer –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                raise
        return cls._model


class RAGSystem:
    def __init__(self, paragraph, collection_name="constitution_articles", document_name='–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è'):
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            logger.warning("‚ö†Ô∏è OPENROUTER_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω! RAG –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.")

        self.client = qdrant_manager.get_client()
        self.api_url = "https://openrouter.ai/api/v1/chat/completions"
        self.llm_model = "google/gemma-3-27b-it:free"  # –û–±–Ω–æ–≤–∏–ª –º–æ–¥–µ–ª—å –Ω–∞ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—É—é free –≤–µ—Ä—Å–∏—é, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        self.codex = paragraph
        self.document_name = document_name
        self.collection_name = collection_name

        self.model = ModelManager.get_model()

        logger.info(f"üîç Init RAG –¥–ª—è: {self.document_name}, –ö–æ–ª–ª–µ–∫—Ü–∏—è: {self.collection_name}")

        self._validate_input_text()
        self.create_embeddings_if_not_exists()

    def _validate_input_text(self):
        if not self.codex or len(self.codex.strip()) < 100:
            logger.error(f"–¢–µ–∫—Å—Ç –¥–ª—è {self.document_name} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π.")
            return

        if "–û –ø—Ä–∞–≤–∞—Ö —Ä–µ–±–µ–Ω–∫–∞" in self.codex and "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è" in self.document_name:
            logger.warning("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–ø—É—Ç–∞–Ω —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è <> –ü—Ä–∞–≤–∞ —Ä–µ–±–µ–Ω–∫–∞).")

    def get_articles_chunks(self):
        if not self.codex:
            return []

        chunks = []
        raw_chunks = re.split(r'(–°—Ç–∞—Ç—å—è \d+\.|–ì–õ–ê–í–ê \d+)', self.codex)

        current_header = self.document_name

        for i in range(len(raw_chunks)):
            segment = raw_chunks[i].strip()
            if not segment:
                continue

            if re.match(r'(–°—Ç–∞—Ç—å—è \d+\.|–ì–õ–ê–í–ê \d+)', segment):
                current_header = segment
            else:
                full_text = f"{current_header}\n{segment}"
                chunks.append({
                    'text': full_text,
                    'article_source': self.document_name,
                    'article_title': current_header,
                    'paragraph_index': i
                })

        logger.info(f"üìÑ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è {self.document_name}")
        return chunks

    def create_embeddings_if_not_exists(self):
        try:
            if not self.client.collection_exists(self.collection_name):
                logger.info(f"üîß –°–æ–∑–¥–∞—é –∫–æ–ª–ª–µ–∫—Ü–∏—é Qdrant: {self.collection_name}")
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),
                )
                self.create_embeddings()
            else:
                count = self.client.count(self.collection_name).count
                if count == 0:
                    logger.info(f"‚ö†Ô∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} –ø—É—Å—Ç–∞, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.")
                    self.create_embeddings()
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant: {e}")

    def create_embeddings(self):
        try:
            chunks = self.get_articles_chunks()
            if not chunks:
                return

            texts = [f"passage: {c['text']}" for c in chunks]

            logger.info("‚è≥ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤...")
            embeddings = self.model.encode(texts, normalize_embeddings=True)

            points = []
            for i, (chunk, vector) in enumerate(zip(chunks, embeddings)):
                points.append(PointStruct(
                    id=i,
                    vector=vector.tolist(),
                    payload=chunk
                ))

            self.client.upsert(collection_name=self.collection_name, points=points)
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ {self.collection_name}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")

    async def search_relevant_chunks_async(self, question, limit=4):
        loop = asyncio.get_running_loop()

        def _encode_and_search():
            query_vector = self.model.encode(f"query: {question}", normalize_embeddings=True)
            search_result = self.client.query_points(
                collection_name=self.collection_name,
                query=query_vector,
                limit=limit
            )
            return search_result

        try:
            result = await loop.run_in_executor(None, _encode_and_search)

            context_parts = []
            for hit in result.points:
                context_parts.append(
                    f"--- {hit.payload.get('article_title', '–û—Ç—Ä—ã–≤–æ–∫')} ---\n{hit.payload.get('text', '')}")

            return "\n\n".join(context_parts)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return ""

    async def answer_question(self, question):
        try:
            context = await self.search_relevant_chunks_async(question)
            if not context:
                return "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö."

            system_prompt = f"–¢—ã —é—Ä–∏—Å—Ç-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É: {self.document_name}. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ —Å—É—Ç–∏, –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–π—Å—è –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞ –≤ 100-200 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–µ –∏—Å–æ–ª—å–∑—É–π —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã Markdown —Ä–∞–∑–º–µ—Ç–∫–∏ —Å—Å—ã–ª–∞—è–π—Å—è –Ω–∞ —Å—Ç–∞—Ç—å–∏. –ü—Ä–µ–¥—Å—Ç–∞–≤—å –æ—Ç–≤–µ—Ç, –≥–æ—Ç–æ–≤—ã–π –∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –≤ telegram"
            user_content = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {question}"

            async with aiohttp.ClientSession() as session:
                payload = {
                    "model": self.llm_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_content}
                    ]
                }
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://telegram.bot",  # –¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ OpenRouter
                }

                async with session.post(self.api_url, json=payload, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data['choices'][0]['message']['content']
                    else:
                        err = await response.text()
                        logger.error(f"LLM Error {response.status}: {err}")
                        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏."

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ RAG: {e}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–∏—Å—Ç–µ–º—ã."


class LegalRAGSystem(RAGSystem):
    def __init__(self, paragraph, document_name):
        safe_name = re.sub(r'[^a-zA-Z0-9_]', '', document_name.lower().replace(' ', '_'))
        collection_name = f"legal_{safe_name}"
        super().__init__(paragraph, collection_name, document_name)